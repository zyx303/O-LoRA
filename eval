+ export CUDA_DEVICE_ORDER=PCI_BUS_ID
+ CUDA_DEVICE_ORDER=PCI_BUS_ID
+ export TRANSFORMERS_CACHE=/data/yongxi/.cache/huggingface
+ TRANSFORMERS_CACHE=/data/yongxi/.cache/huggingface
++ shuf -i25000-30000 -n1
+ port=26516
+ CUDA_VISIBLE_DEVICES=7
+ deepspeed --master_port 26516 src/run_uie_lora.py --do_predict --predict_with_generate --model_name_or_path logs_and_outputs/sdlora/order_1/outputs/1-dbpedia/adapter --data_dir CL_Benchmark --task_config_dir configs/order1_configs/amazon --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/eval/order_1/outputs/2-amazon --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 1 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round2_sdlora --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0 --peft_type SDLORA
[2025-08-28 21:00:05,576] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2025-08-28 21:00:06,619] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=7: setting --include=localhost:7
[2025-08-28 21:00:06,697] [INFO] [runner.py:555:main] cmd = /home/yongxi/miniconda3/envs/sd-lora/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbN119 --master_addr=127.0.0.1 --master_port=26516 --enable_each_rank_log=None src/run_uie_lora.py --do_predict --predict_with_generate --model_name_or_path logs_and_outputs/sdlora/order_1/outputs/1-dbpedia/adapter --data_dir CL_Benchmark --task_config_dir configs/order1_configs/amazon --instruction_file configs/instruction_config.json --instruction_strategy single --output_dir logs_and_outputs/eval/order_1/outputs/2-amazon --per_device_train_batch_size 8 --per_device_eval_batch_size 128 --gradient_accumulation_steps 1 --learning_rate 1e-03 --num_train_epochs 1 --deepspeed configs/ds_configs/stage2.config --run_name order1_round2_sdlora --max_source_length 512 --max_target_length 50 --generation_max_length 50 --add_task_name True --add_dataset_name True --overwrite_output_dir --overwrite_cache --lr_scheduler_type constant --warmup_steps 0 --logging_strategy steps --logging_steps 10 --evaluation_strategy no --save_strategy no --save_steps 1500 --lamda_1 0.5 --lamda_2 0 --peft_type SDLORA
[2025-08-28 21:00:07,822] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[2025-08-28 21:00:09,181] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [7]}
[2025-08-28 21:00:09,181] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-08-28 21:00:09,182] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-08-28 21:00:09,182] [INFO] [launch.py:163:main] dist_world_size=1
[2025-08-28 21:00:09,182] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=7
[2025-08-28 21:00:11,373] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
[2025-08-28 21:00:12,208] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-08-28 21:00:12,208] [INFO] [comm.py:616:init_distributed] cdb=None
[2025-08-28 21:00:12,208] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
08/28/2025 21:00:12 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
08/28/2025 21:00:12 - WARNING - datasets.builder - Using custom data configuration default-25178e251dd03be0
08/28/2025 21:00:12 - WARNING - datasets.builder - Reusing dataset uie_instructions (logs_and_outputs/eval/order_1/outputs/2-amazon/f236d5f0073f6b88182545d96de70db6/uie_instructions/default-25178e251dd03be0/2.0.0/c490e7f13dec80785fc335819009163a45c86ae2816040c8d81800108e7e4374)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 779.56it/s]
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
-----Gradient checkpointing: False -----
[93m [WARNING] [0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!
Using /home/yongxi/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /home/yongxi/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4342243671417236 seconds
Rank: 0 partition count [1] and sizes[(2359440, False)] 
[rank0]:[W828 21:00:24.816319400 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
/home/yongxi/miniconda3/envs/sd-lora/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
  0%|          | 0/119 [00:00<?, ?it/s]  2%|▏         | 2/119 [00:01<01:22,  1.42it/s]  3%|▎         | 3/119 [00:02<01:56,  1.00s/it]  3%|▎         | 4/119 [00:04<02:11,  1.14s/it]  4%|▍         | 5/119 [00:05<02:31,  1.33s/it]  5%|▌         | 6/119 [00:07<02:34,  1.37s/it]  6%|▌         | 7/119 [00:08<02:35,  1.39s/it]  7%|▋         | 8/119 [00:10<02:36,  1.41s/it]  8%|▊         | 9/119 [00:11<02:39,  1.45s/it]  8%|▊         | 10/119 [00:13<02:41,  1.48s/it]  9%|▉         | 11/119 [00:14<02:36,  1.45s/it] 10%|█         | 12/119 [00:16<02:38,  1.48s/it] 11%|█         | 13/119 [00:17<02:34,  1.46s/it] 12%|█▏        | 14/119 [00:19<02:30,  1.43s/it] 13%|█▎        | 15/119 [00:20<02:27,  1.42s/it] 13%|█▎        | 16/119 [00:21<02:25,  1.41s/it] 14%|█▍        | 17/119 [00:23<02:21,  1.39s/it] 15%|█▌        | 18/119 [00:25<02:37,  1.56s/it] 16%|█▌        | 19/119 [00:26<02:46,  1.66s/it] 17%|█▋        | 20/119 [00:28<02:42,  1.64s/it] 18%|█▊        | 21/119 [00:29<02:33,  1.56s/it] 18%|█▊        | 22/119 [00:31<02:26,  1.51s/it] 19%|█▉        | 23/119 [00:32<02:22,  1.48s/it] 20%|██        | 24/119 [00:34<02:17,  1.45s/it] 21%|██        | 25/119 [00:35<02:17,  1.46s/it] 22%|██▏       | 26/119 [00:37<02:15,  1.46s/it] 23%|██▎       | 27/119 [00:38<02:23,  1.56s/it] 24%|██▎       | 28/119 [00:40<02:16,  1.50s/it] 24%|██▍       | 29/119 [00:42<02:25,  1.61s/it] 25%|██▌       | 30/119 [00:43<02:17,  1.54s/it] 26%|██▌       | 31/119 [00:44<02:12,  1.51s/it] 27%|██▋       | 32/119 [00:46<02:10,  1.50s/it] 28%|██▊       | 33/119 [00:47<02:05,  1.46s/it] 29%|██▊       | 34/119 [00:49<02:01,  1.43s/it] 29%|██▉       | 35/119 [00:50<01:58,  1.42s/it] 30%|███       | 36/119 [00:51<01:58,  1.43s/it] 31%|███       | 37/119 [00:53<01:55,  1.41s/it] 32%|███▏      | 38/119 [00:54<01:52,  1.39s/it] 33%|███▎      | 39/119 [00:56<01:52,  1.41s/it] 34%|███▎      | 40/119 [00:57<01:51,  1.41s/it] 34%|███▍      | 41/119 [00:59<01:51,  1.43s/it] 35%|███▌      | 42/119 [01:00<01:48,  1.41s/it] 36%|███▌      | 43/119 [01:01<01:46,  1.40s/it] 37%|███▋      | 44/119 [01:03<01:40,  1.34s/it] 38%|███▊      | 45/119 [01:04<01:46,  1.44s/it] 39%|███▊      | 46/119 [01:06<01:56,  1.59s/it] 39%|███▉      | 47/119 [01:08<01:54,  1.58s/it] 40%|████      | 48/119 [01:09<01:55,  1.62s/it] 41%|████      | 49/119 [01:11<01:48,  1.55s/it] 42%|████▏     | 50/119 [01:12<01:46,  1.55s/it] 43%|████▎     | 51/119 [01:14<01:42,  1.51s/it] 44%|████▎     | 52/119 [01:15<01:38,  1.46s/it] 45%|████▍     | 53/119 [01:17<01:35,  1.45s/it] 45%|████▌     | 54/119 [01:18<01:38,  1.51s/it] 46%|████▌     | 55/119 [01:20<01:38,  1.55s/it] 47%|████▋     | 56/119 [01:21<01:37,  1.54s/it] 48%|████▊     | 57/119 [01:23<01:42,  1.65s/it] 49%|████▊     | 58/119 [01:25<01:38,  1.61s/it] 50%|████▉     | 59/119 [01:26<01:34,  1.57s/it] 50%|█████     | 60/119 [01:28<01:33,  1.59s/it] 51%|█████▏    | 61/119 [01:29<01:24,  1.46s/it] 52%|█████▏    | 62/119 [01:31<01:26,  1.51s/it] 53%|█████▎    | 63/119 [01:32<01:19,  1.42s/it] 54%|█████▍    | 64/119 [01:33<01:13,  1.34s/it] 55%|█████▍    | 65/119 [01:34<01:11,  1.32s/it] 55%|█████▌    | 66/119 [01:36<01:09,  1.31s/it] 56%|█████▋    | 67/119 [01:37<01:06,  1.28s/it] 57%|█████▋    | 68/119 [01:38<01:04,  1.26s/it] 58%|█████▊    | 69/119 [01:39<01:02,  1.25s/it] 59%|█████▉    | 70/119 [01:40<00:59,  1.22s/it] 60%|█████▉    | 71/119 [01:42<00:58,  1.22s/it] 61%|██████    | 72/119 [01:43<00:56,  1.20s/it] 61%|██████▏   | 73/119 [01:44<00:54,  1.18s/it] 62%|██████▏   | 74/119 [01:45<00:51,  1.15s/it] 63%|██████▎   | 75/119 [01:46<00:49,  1.14s/it] 64%|██████▍   | 76/119 [01:47<00:51,  1.20s/it] 65%|██████▍   | 77/119 [01:49<00:49,  1.17s/it] 66%|██████▌   | 78/119 [01:50<00:46,  1.14s/it] 66%|██████▋   | 79/119 [01:51<00:45,  1.14s/it] 67%|██████▋   | 80/119 [01:52<00:46,  1.18s/it] 68%|██████▊   | 81/119 [01:53<00:45,  1.20s/it] 69%|██████▉   | 82/119 [01:54<00:44,  1.20s/it] 70%|██████▉   | 83/119 [01:56<00:44,  1.23s/it] 71%|███████   | 84/119 [01:57<00:43,  1.25s/it] 71%|███████▏  | 85/119 [02:00<01:00,  1.77s/it] 72%|███████▏  | 86/119 [02:01<00:52,  1.60s/it] 73%|███████▎  | 87/119 [02:02<00:46,  1.46s/it] 74%|███████▍  | 88/119 [02:04<00:46,  1.50s/it] 75%|███████▍  | 89/119 [02:05<00:42,  1.41s/it] 76%|███████▌  | 90/119 [02:06<00:38,  1.32s/it] 76%|███████▋  | 91/119 [02:07<00:35,  1.25s/it] 77%|███████▋  | 92/119 [02:08<00:32,  1.21s/it] 78%|███████▊  | 93/119 [02:10<00:31,  1.19s/it] 79%|███████▉  | 94/119 [02:11<00:29,  1.18s/it] 80%|███████▉  | 95/119 [02:12<00:28,  1.17s/it] 81%|████████  | 96/119 [02:13<00:26,  1.16s/it] 82%|████████▏ | 97/119 [02:14<00:25,  1.14s/it] 82%|████████▏ | 98/119 [02:15<00:24,  1.15s/it] 83%|████████▎ | 99/119 [02:16<00:23,  1.16s/it] 84%|████████▍ | 100/119 [02:18<00:22,  1.16s/it] 85%|████████▍ | 101/119 [02:19<00:21,  1.18s/it] 86%|████████▌ | 102/119 [02:22<00:29,  1.71s/it] 87%|████████▋ | 103/119 [02:23<00:25,  1.57s/it] 87%|████████▋ | 104/119 [02:24<00:21,  1.47s/it] 88%|████████▊ | 105/119 [02:25<00:19,  1.37s/it] 89%|████████▉ | 106/119 [02:27<00:16,  1.30s/it] 90%|████████▉ | 107/119 [02:28<00:15,  1.28s/it] 91%|█████████ | 108/119 [02:29<00:14,  1.34s/it] 92%|█████████▏| 109/119 [02:31<00:14,  1.41s/it] 92%|█████████▏| 110/119 [02:32<00:12,  1.40s/it] 93%|█████████▎| 111/119 [02:33<00:10,  1.35s/it] 94%|█████████▍| 112/119 [02:35<00:09,  1.34s/it] 95%|█████████▍| 113/119 [02:36<00:07,  1.31s/it] 96%|█████████▌| 114/119 [02:37<00:06,  1.27s/it] 97%|█████████▋| 115/119 [02:39<00:05,  1.32s/it] 97%|█████████▋| 116/119 [02:40<00:03,  1.29s/it] 98%|█████████▊| 117/119 [02:41<00:02,  1.26s/it] 99%|█████████▉| 118/119 [02:42<00:01,  1.28s/it]100%|██████████| 119/119 [02:43<00:00,  1.19s/it]100%|██████████| 119/119 [02:50<00:00,  1.43s/it]
***** predict metrics *****
  predict_exact_match             =    61.4737
  predict_exact_match_for_SC      =    24.1053
  predict_exact_match_for_TC      =    98.8421
  predict_exact_match_for_amazon  =    24.1053
  predict_exact_match_for_dbpedia =    98.8421
  predict_gen_len                 =     2.3428
  predict_global_step             =          0
  predict_loss                    =     0.6582
  predict_rouge1                  =    70.2621
  predict_rouge1_for_SC           =     41.682
  predict_rouge1_for_TC           =    98.8421
  predict_rouge1_for_amazon       =     41.682
  predict_rouge1_for_dbpedia      =    98.8421
  predict_rougeL                  =    70.2621
  predict_rougeL_for_SC           =     41.682
  predict_rougeL_for_TC           =    98.8421
  predict_rougeL_for_amazon       =     41.682
  predict_rougeL_for_dbpedia      =    98.8421
  predict_runtime                 = 0:02:57.25
  predict_samples                 =      15200
  predict_samples_per_second      =     85.751
  predict_steps_per_second        =      0.671
[rank0]:[W828 21:03:16.576624353 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-08-28 21:03:18,377] [INFO] [launch.py:347:main] Process 1724238 exits successfully.
