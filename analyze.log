=== SDLoRA Adapter Analysis ===

Analyzing task: 1-dbpedia
--------------------------------------------------
Config: {
  "base_model_name_or_path": "initial_model/t5-large",
  "bias": "none",
  "fan_in_fan_out": false,
  "inference_mode": true,
  "init_lora_weights": true,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "modules_to_save": null,
  "peft_type": "SDLORA",
  "r": 8,
  "r_sum": 0,
  "save_loranew": true,
  "target_modules": [
    "q",
    "v"
  ],
  "task_type": "SEQ_2_SEQ_LM"
}

Weight keys in 1-dbpedia:
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.num_historical_directions value: 1
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.num_historical_directions value: 1
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.num_historical_directions value: 1
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.num_historical_directions value: 1
Number of layers with LoRA: 0

======================================================================

Analyzing task: 2-amazon
--------------------------------------------------
Config: {
  "base_model_name_or_path": "initial_model/t5-large",
  "bias": "none",
  "fan_in_fan_out": false,
  "inference_mode": true,
  "init_lora_weights": true,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "modules_to_save": null,
  "peft_type": "SDLORA",
  "r": 8,
  "r_sum": 0,
  "save_loranew": true,
  "target_modules": [
    "q",
    "v"
  ],
  "task_type": "SEQ_2_SEQ_LM"
}

Weight keys in 2-amazon:
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_scalings.dir_0: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.historical_scalings.dir_1: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.num_historical_directions: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.num_historical_directions value: 2
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_scalings.dir_0: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.historical_scalings.dir_1: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.num_historical_directions: []
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.num_historical_directions value: 2
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_scalings.dir_0: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.historical_scalings.dir_1: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.num_historical_directions: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.num_historical_directions value: 2
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_0.A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_0.B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_1.A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_directions.dir_1.B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_scalings.dir_0: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.historical_scalings.dir_1: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.num_historical_directions: []
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.num_historical_directions value: 2
Number of layers with LoRA: 0

======================================================================

Analyzing task: 3-yahoo
--------------------------------------------------
Config: {
  "base_model_name_or_path": "initial_model/t5-large",
  "bias": "none",
  "fan_in_fan_out": false,
  "inference_mode": true,
  "init_lora_weights": true,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "modules_to_save": null,
  "peft_type": "SDLORA",
  "r": 8,
  "r_sum": 0,
  "save_loranew": true,
  "target_modules": [
    "q",
    "v"
  ],
  "task_type": "SEQ_2_SEQ_LM"
}

Weight keys in 3-yahoo:
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
Number of layers with LoRA: 0

======================================================================

Analyzing task: 4-agnews
--------------------------------------------------
Config: {
  "base_model_name_or_path": "initial_model/t5-large",
  "bias": "none",
  "fan_in_fan_out": false,
  "inference_mode": true,
  "init_lora_weights": true,
  "lora_alpha": 32,
  "lora_dropout": 0.1,
  "modules_to_save": null,
  "peft_type": "SDLORA",
  "r": 8,
  "r_sum": 0,
  "save_loranew": true,
  "target_modules": [
    "q",
    "v"
  ],
  "task_type": "SEQ_2_SEQ_LM"
}

Weight keys in 4-agnews:
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.0.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.1.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.10.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.11.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.12.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.13.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.14.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.15.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.16.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.17.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.18.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.19.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.2.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.20.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.21.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.22.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.23.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.3.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.4.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.5.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.6.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.7.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.8.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.weight: [0, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.weight: [1024, 0]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.decoder.block.9.layer.1.EncDecAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.0.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.1.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.10.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.11.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.12.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.13.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.14.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.15.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.16.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.17.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.18.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.19.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.2.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.20.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.21.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.22.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.23.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.3.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.4.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.5.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.6.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.7.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.8.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.q.loranew_B.weight: [1024, 8]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_A.weight: [0, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.lora_B.weight: [1024, 0]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.loranew_A.weight: [8, 1024]
  base_model.model.encoder.block.9.layer.0.SelfAttention.v.loranew_B.weight: [1024, 8]
Number of layers with LoRA: 0

======================================================================

Detailed analysis saved to: /home/yongxi/work/O-LoRA/adapter_analysis.json
=== Task Comparison ===

Total parameters per task:

Rank parameters:

Weight statistics across tasks:
=== LoRA Directions Analysis ===

